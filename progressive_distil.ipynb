{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progressive distillation on sparse parity\n",
    "This is a code for the paper \"Progressive distillation induces an implicit curriculum\". Full code is available here: https://github.com/abhishekpanigrahi1996/ProgressiveDistillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the relevant packages\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch \n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary Utils for the rest of the code\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    print(f\"Random seed set as {seed}\")\n",
    "    \n",
    "\"\"\"\n",
    "Model related\n",
    "\"\"\"\n",
    "\n",
    "def load_torch_ckpt(model, fckpt):\n",
    "  ckpt = torch.load(fckpt, map_location=DEVICE)\n",
    "  model.load_state_dict(ckpt['model_state_dict'])\n",
    "  print(\"Loaded ckpt\", fckpt)\n",
    "  return model\n",
    "\n",
    "def get_mlp(n_input, n_hidden, n_output, n_layers):\n",
    "  layers = []\n",
    "  for i in range(n_layers):\n",
    "      if i == 0:\n",
    "          layers.append(torch.nn.Linear(n_input, n_hidden))\n",
    "      else:\n",
    "          layers.append(torch.nn.Linear(n_hidden, n_hidden))\n",
    "      layers.append(torch.nn.ReLU())\n",
    "  layers.append(torch.nn.Linear(n_hidden, n_output))\n",
    "  return torch.nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "relu = torch.nn.ReLU()\n",
    "\n",
    "def get_logits(input_, layers, return_hidden=0):\n",
    "    num_layers = len(layers) - 1\n",
    "    out = input_\n",
    "    hiddens = {}\n",
    "    for li in range(num_layers):\n",
    "        out = relu(layers[li](out))\n",
    "        if return_hidden:\n",
    "            hiddens[li] = out.detach().cpu().np()\n",
    "    out = layers[num_layers](out)\n",
    "    if return_hidden:\n",
    "        return out, hiddens\n",
    "    return out\n",
    "\n",
    "def get_logits_gpt(input_, model):\n",
    "    output = model(input_ids=input_)\n",
    "    logits = output.logits\n",
    "    return logits[:, -1, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Hierarchical tree data\n",
    "\"\"\"\n",
    "\n",
    "def get_features(num_labels, d, feature_complexity, random=False, feature_coordinates=None):\n",
    "    if random:\n",
    "        all_features = np.random.choice(d, size=(num_labels-1, feature_complexity))\n",
    "    else:\n",
    "        assert (num_labels-1) * feature_complexity <= d, \"Number of available components should be more\"\n",
    "        if feature_coordinates is None:\n",
    "            all_features = [range(i, i+feature_complexity) for i in range(0, (num_labels-1)*feature_complexity, feature_complexity)]\n",
    "        else:\n",
    "            all_features = [feature_coordinates]\n",
    "    return all_features\n",
    "\n",
    "\n",
    "def boolean_data(n, d, num_labels, all_features):\n",
    "    def score(train_data, all_features, label):\n",
    "        all_scores = np.zeros((len(train_data),))\n",
    "        \n",
    "        while(label > 1):\n",
    "            prod_features = all_features [label//2-1]\n",
    "            score_ = (1 - 2*(label%2)) * np.prod(train_data[:, prod_features], axis=-1)\n",
    "            all_scores += score_\n",
    "            label = label // 2\n",
    "            \n",
    "        return all_scores\n",
    "    \n",
    "    train_x = 2*np.random.choice(2, size=(n, d))-1\n",
    "    \n",
    "    train_y = np.zeros((n, num_labels))\n",
    "    for i in range(num_labels):\n",
    "        train_y[:, i] = score(train_x, all_features, i+num_labels)\n",
    "    \n",
    "    return train_x, np.argmax(train_y, axis=-1).astype(np.int32)\n",
    "\n",
    " \n",
    "class HierarchicalData(Dataset):\n",
    "  def __init__(self, data, labels):\n",
    "      self.inputs = data\n",
    "      self.labels = labels\n",
    "      self.n_examples = len(data)\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "      return self.inputs[idx], self.labels[idx]\n",
    "\n",
    "  def __len__(self):\n",
    "      return self.n_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "Data related\n",
    "\"\"\"\n",
    "\n",
    "def get_data_loaders(cfg, seed):\n",
    "    set_seed(seed)\n",
    "\n",
    "    data_type = cfg['data']['data_type']\n",
    "    num_labels = cfg['data']['num_labels']\n",
    "    num_workers = cfg['data']['num_workers']\n",
    "    model_type = cfg['model']['type']\n",
    "\n",
    "    if data_type == 'hierarchical':\n",
    "        data_dimension = cfg['data']['data_dimension']\n",
    "        feature_complexity = cfg['data']['feature_complexity']\n",
    "        randomize_features = cfg['data']['randomize_features']\n",
    "        n_examples = cfg['training']['n_examples']\n",
    "        batch_size = cfg['training']['batch_size']\n",
    "\n",
    "        all_features = get_features(num_labels, data_dimension, feature_complexity, random=randomize_features)\n",
    "        # here, we set the seed to make deterministic runs\n",
    "        all_data, all_y = boolean_data(n_examples, data_dimension, num_labels, all_features)\n",
    "        \n",
    "\n",
    "        eval_split = num_labels * min(2048, len(all_data)//4)\n",
    "        train_split = len(all_data) - eval_split\n",
    "        eval_batch_size = min(1000, eval_split)\n",
    "        \n",
    "        train_data, train_y = all_data[:train_split], all_y[:train_split]\n",
    "        train_dataset = HierarchicalData(train_data, train_y)\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                                  shuffle=True, num_workers=num_workers)\n",
    "        eval_data, eval_y = all_data[train_split:], all_y[train_split:]\n",
    "        eval_dataset = HierarchicalData(eval_data, eval_y)\n",
    "        eval_loader = torch.utils.data.DataLoader(eval_dataset, batch_size=eval_batch_size,\n",
    "                                                  shuffle=False, num_workers=num_workers)\n",
    "    return train_loader, eval_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "Training related\n",
    "\"\"\"\n",
    "def loss_fn(pred, target): \n",
    "    loss_ = torch.nn.CrossEntropyLoss()\n",
    "    return loss_ (pred, target)\n",
    "\n",
    "def accuracy(pred, target):\n",
    "    return (torch.argmax(pred, axis=-1) == target).type(torch.float32).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Run SFT experiments to compare models of different sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the training function\n",
    "\n",
    "def train(hidden_size, \n",
    "          model_type, \n",
    "          data_dimension, \n",
    "          num_labels, \n",
    "          learning_rate, \n",
    "          weight_decay, \n",
    "          output_path, \n",
    "          seed, \n",
    "          num_layers, \n",
    "          feature_complexity, \n",
    "          randomize_features, \n",
    "          feature_coordinates, \n",
    "          n_examples, \n",
    "          n_steps, \n",
    "          n_epochs, \n",
    "          batch_size, \n",
    "          eval_batch_size, \n",
    "          log_intvl, \n",
    "          save_intvl,\n",
    "          warmup_ratio,\n",
    "          anneal_type\n",
    "        ):\n",
    "\n",
    "\n",
    "    save_suffix = f'_{model_type}_hid{hidden_size}' + f'_n{data_dimension}_k{feature_complexity}' + f'_num_labels{num_labels}' + f'_lr{learning_rate}' + f'warm{warmup_ratio}' + f'_wd{weight_decay}' + f'_seed{seed}' + f'_num_layers{num_layers}' + f'_e{n_epochs}'\n",
    "    output_dir = output_path + save_suffix\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Get data\n",
    "    \"\"\"\n",
    "    set_seed(seed)\n",
    "    all_features = get_features(num_labels, data_dimension, feature_complexity,\n",
    "                                random=randomize_features,\n",
    "                                feature_coordinates=feature_coordinates,)\n",
    "    all_data, all_y = boolean_data(n_examples, data_dimension, num_labels, all_features)\n",
    "  \n",
    "    eval_split = num_labels * 125\n",
    "    train_split = len(all_data) - eval_split\n",
    "    eval_batch_size = min(1000, eval_split)\n",
    "    \n",
    "    train_data, train_y = all_data[:train_split], all_y[:train_split]\n",
    "    train_dataset = HierarchicalData(train_data, train_y)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                               shuffle=True, num_workers=1)\n",
    "    eval_data, eval_y = all_data[train_split:], all_y[train_split:]\n",
    "    eval_dataset = HierarchicalData(eval_data, eval_y)\n",
    "    eval_loader = torch.utils.data.DataLoader(eval_dataset, batch_size=eval_batch_size,\n",
    "                                              shuffle=False, num_workers=1)\n",
    "\n",
    "    dtype = torch.float32 if model_type == 'mlp' else torch.long\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Get model\n",
    "    \"\"\"\n",
    "    all_layers, all_parameters = get_mlp(data_dimension, num_labels, hidden_size, num_layers)\n",
    "    optimizer = torch.optim.SGD(all_parameters, lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "    global_step_cnt = 0\n",
    "    eval_accs = []\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        print(\"Current Epoch:\", epoch)\n",
    "        for bt, batch in tqdm(enumerate(train_loader)):\n",
    "            batch_data, batch_y = batch\n",
    "\n",
    "            cuda_batch_data, cuda_batch_y = batch_data.to(DEVICE).type(dtype), batch_y.to(DEVICE).long()\n",
    "            \n",
    "            predicted_y = all_layers(cuda_batch_data)\n",
    "            \n",
    "            loss = loss_fn(predicted_y, cuda_batch_y)\n",
    "            loss.backward()\n",
    "\n",
    "            # set learning rate\n",
    "            curr_learning_rate = 0\n",
    "            if global_step_cnt < n_steps * warmup_ratio:\n",
    "                # linear warmup\n",
    "                curr_learning_rate = learning_rate * (global_step_cnt+1) / (n_steps * warmup_ratio)\n",
    "            else:\n",
    "                if anneal_type == 'cosine':\n",
    "                    curr_learning_rate = learning_rate * 0.5 * (1 + np.cos(np.pi * (global_step_cnt - n_steps * warmup_ratio) / (n_steps * (1-warmup_ratio))))\n",
    "                elif anneal_type == 'linear':\n",
    "                    curr_learning_rate = learning_rate * (1 - (global_step_cnt - n_steps * warmup_ratio) / (n_steps * (1-warmup_ratio)))\n",
    "                elif anneal_type == 'constant':\n",
    "                    curr_learning_rate = learning_rate\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = curr_learning_rate \n",
    "\n",
    "\n",
    "            if global_step_cnt % 5000 == 0:\n",
    "               print(f\"Loss at step {global_step_cnt}: {loss.item()}\")\n",
    "\n",
    "                                \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if global_step_cnt % min(log_intvl, len(train_loader)) == 0:\n",
    "                eval_loss = 0.\n",
    "                predicted_y_gap = 0\n",
    "                for bt, batch in tqdm(enumerate(eval_loader)):\n",
    "                    batch_data, batch_y = batch\n",
    "                    cuda_batch_data, cuda_batch_y = batch_data.to(DEVICE).type(dtype), batch_y.to(DEVICE).long()\n",
    "                    with torch.no_grad():\n",
    "                        eval_logits = all_layers(cuda_batch_data)\n",
    "                        predicted_y = torch.nn.functional.softmax(eval_logits, dim=-1)\n",
    "                        predicted_y_gap += (predicted_y[:, 0] -  predicted_y[:, 1]).mean().item()\n",
    "                        loss = accuracy(predicted_y, cuda_batch_y)\n",
    "                        eval_loss += loss.item()\n",
    "                eval_acc = eval_loss / len(eval_loader)\n",
    "                eval_accs += eval_acc,\n",
    "                predicted_y_gap /= len(eval_loader)\n",
    "                \n",
    "                # check for early stopping\n",
    "                if len(eval_accs) > 10 and sum(eval_accs[-10:])/10 > 0.9999 and 0:\n",
    "                    print(\"Early Stopping at:\", global_step_cnt)\n",
    "                    return\n",
    "                \n",
    "            if global_step_cnt % save_intvl == 0:\n",
    "                if not os.path.exists(output_dir):\n",
    "                    os.makedirs(output_dir)\n",
    "                print(\"Save at:\", global_step_cnt)\n",
    "                fout = output_dir + '/checkpoint-'+str(global_step_cnt)\n",
    "                \n",
    "                if type(all_layers) == list:\n",
    "                  model_state_dict = [layer.state_dict() for layer in all_layers]\n",
    "                else:\n",
    "                  model_state_dict = all_layers.state_dict()\n",
    "                try:\n",
    "                  torch.save(\n",
    "                      {\n",
    "                      'epoch': epoch, \n",
    "                      'step': global_step_cnt,\n",
    "                      'model_state_dict': model_state_dict,\n",
    "                      'optimizer_state_dict': optimizer.state_dict()\n",
    "                    },\n",
    "                    fout)\n",
    "                except:\n",
    "                    print(\"Failed to save model\")\n",
    "            global_step_cnt += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 26\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# get the training arguments\u001b[39;00m\n\u001b[1;32m      2\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m10_000\u001b[39m,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmlp\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124manneal_type\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcosine\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     24\u001b[0m }\n\u001b[0;32m---> 26\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 38\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(hidden_size, model_type, data_dimension, num_labels, learning_rate, weight_decay, output_path, seed, num_layers, feature_complexity, randomize_features, feature_coordinates, n_examples, n_steps, n_epochs, batch_size, eval_batch_size, log_intvl, save_intvl, warmup_ratio, anneal_type)\u001b[0m\n\u001b[1;32m     34\u001b[0m set_seed(seed)\n\u001b[1;32m     35\u001b[0m all_features \u001b[38;5;241m=\u001b[39m get_features(num_labels, data_dimension, feature_complexity,\n\u001b[1;32m     36\u001b[0m                             random\u001b[38;5;241m=\u001b[39mrandomize_features,\n\u001b[1;32m     37\u001b[0m                             feature_coordinates\u001b[38;5;241m=\u001b[39mfeature_coordinates,)\n\u001b[0;32m---> 38\u001b[0m all_data, all_y \u001b[38;5;241m=\u001b[39m \u001b[43mboolean_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_examples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_dimension\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m eval_split \u001b[38;5;241m=\u001b[39m num_labels \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m125\u001b[39m\n\u001b[1;32m     41\u001b[0m train_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_data) \u001b[38;5;241m-\u001b[39m eval_split\n",
      "Cell \u001b[0;32mIn[9], line 33\u001b[0m, in \u001b[0;36mboolean_data\u001b[0;34m(n, d, num_labels, all_features)\u001b[0m\n\u001b[1;32m     31\u001b[0m train_y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((n, num_labels))\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_labels):\n\u001b[0;32m---> 33\u001b[0m     train_y[:, i] \u001b[38;5;241m=\u001b[39m \u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mnum_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m train_x, np\u001b[38;5;241m.\u001b[39margmax(train_y, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint32)\n",
      "Cell \u001b[0;32mIn[9], line 23\u001b[0m, in \u001b[0;36mboolean_data.<locals>.score\u001b[0;34m(train_data, all_features, label)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m(label \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     22\u001b[0m     prod_features \u001b[38;5;241m=\u001b[39m all_features [label\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 23\u001b[0m     score_ \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m(label\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m2\u001b[39m)) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mprod(\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprod_features\u001b[49m\u001b[43m]\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     24\u001b[0m     all_scores \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m score_\n\u001b[1;32m     25\u001b[0m     label \u001b[38;5;241m=\u001b[39m label \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "# get the training arguments\n",
    "config = {\n",
    "    'hidden_size': 10_000,\n",
    "    'model_type': 'mlp',\n",
    "    'data_dimension': 50,\n",
    "    'num_labels': 2,\n",
    "    'learning_rate': 1e-2,\n",
    "    'weight_decay': 0.05,\n",
    "    'output_path': 'result',\n",
    "    'seed': 0,\n",
    "    'num_layers': 2,\n",
    "    'feature_complexity': 3,\n",
    "    'randomize_features': False,\n",
    "    'feature_coordinates': '',\n",
    "    'n_examples': 100_000,\n",
    "    'n_steps': 100_000,\n",
    "    'n_epochs': 1,\n",
    "    'batch_size': 1,\n",
    "    'eval_batch_size': 128,\n",
    "    'log_intvl': 10_000,\n",
    "    'save_intvl': 10_000,\n",
    "    'warmup_ratio': 0.05,\n",
    "    'anneal_type': 'cosine',\n",
    "}\n",
    "\n",
    "train(**config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
