{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SFT on GSM8k\n",
    "This code provides a simplistic training recipe for running supervised fine-tuning on a Llama 3.2 3B model on GSM8k dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install any packages if necessary by !pip install .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import relevant packages\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, TrainingArguments, AutoModelForCausalLM\n",
    "from datasets import Dataset\n",
    "\n",
    "from trl import SFTTrainer, SFTConfig, DataCollatorForCompletionOnlyLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b7afeb577774ca79869a7d9349804fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## mention a cache dir if you want to save a local file\n",
    "cache_dir=''\n",
    "\n",
    "model_name = \"Path to Llama-3.2-3B\" ### change here for other models\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)  ### load tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the relevant dataset and create a train/test split\n",
    "\n",
    "data = load_dataset('openai/gsm8k', 'main', cache_dir=cache_dir)\n",
    "\n",
    "train_data = data['train']\n",
    "test_data = data['test']\n",
    "\n",
    "def create_conversation(d):\n",
    "    text = '### Question:\\n' + str(d['question']) + '\\n\\n### Solution:\\n' + str(d['answer'])\n",
    "    return {'text': text}\n",
    "\n",
    "conversation_train_data = []\n",
    "for d in train_data:\n",
    "    conversation_train_data += [create_conversation(d)]\n",
    "conversation_train_data = Dataset.from_list(conversation_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the training arguments\n",
    "output_dir='result'\n",
    "\n",
    "num_train_epochs=3\n",
    "per_device_train_batch_size=1\n",
    "gradient_accumulation_steps=64\n",
    "gradient_checkpointing=False\n",
    "optim = 'adamw_torch'\n",
    "\n",
    "logging_steps=10\n",
    "save_strategy='epoch'\n",
    "bf16='True'\n",
    "\n",
    "learning_rate=5e-5\n",
    "weight_decay=0.0\n",
    "\n",
    "max_sequence_length=512\n",
    "\n",
    "warmup_ratio=0.03\n",
    "lr_scheduler_type='cosine'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we define the arguments for the Trainer\n",
    "\n",
    "# We first start with a data collator that computes loss only on the answer tokens\n",
    "response_template = \"### Solution:\\n\" \n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n",
    "\n",
    "# Next we define the configuration of the trainer with relevant arguments\n",
    "args = SFTConfig(\n",
    "    output_dir=output_dir,  # directory to save and repository id\n",
    "    num_train_epochs=num_train_epochs,  # number of training epochs\n",
    "    per_device_train_batch_size=per_device_train_batch_size,  # batch size per device during training\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,  # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=gradient_checkpointing,  # use gradient checkpointing to save memory\n",
    "    optim=optim,  # use fused adamw optimizer\n",
    "    logging_steps=logging_steps,  # log every 10 steps\n",
    "    save_strategy=save_strategy,  # save checkpoint every epoch\n",
    "    bf16=bf16,  # use bfloat16 precision\n",
    "    learning_rate=learning_rate,  # learning rate, based on QLoRA paper\n",
    "    weight_decay=weight_decay,\n",
    "    max_seq_length=max_sequence_length,\n",
    "    warmup_ratio=warmup_ratio,  # warmup ratio based on QLoRA paper\n",
    "    lr_scheduler_type=lr_scheduler_type,  # use constant learning rate scheduler\n",
    "    report_to='tensorboard',\n",
    ")\n",
    "\n",
    "config = {}\n",
    "config['model'] = model\n",
    "config['processing_class'] = tokenizer\n",
    "config['train_dataset'] = conversation_train_data\n",
    "\n",
    "config['args'] = args\n",
    "config['data_collator'] = collator\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(**config)\n",
    "\n",
    "train_result = trainer.train()\n",
    "metrics = train_result.metrics\n",
    "\n",
    "metrics[\"train_samples\"] = len(conversation_train_data)\n",
    "\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()\n",
    "\n",
    "\n",
    "trainer.save_model(output_dir)  # Saves model & weights\n",
    "\n",
    "# Explicitly save the tokenizer\n",
    "tokenizer.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now evaluate by generating the responses\n",
    "from transformers import GenerationConfig\n",
    "import re\n",
    "from sympy import sympify, Eq, simplify\n",
    "\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    max_length=max_sequence_length,\n",
    "    temperature=0.0,\n",
    "    top_p=0.95,\n",
    "    do_sample=False,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "def create_question(d):\n",
    "    text = '### Question:\\n' + str(d['question']) + '\\n\\n### Solution:\\n' \n",
    "    return {'text': text, 'answer': d['answer']}\n",
    "\n",
    "conversation_test_data = []\n",
    "for d in test_data:\n",
    "    conversation_test_data += [create_question(d)]\n",
    "conversation_test_data =  Dataset.from_list(conversation_test_data)\n",
    "\n",
    "\n",
    "def extract_last_number(text):\n",
    "    numbers = re.findall(r\"-?\\d+(?:\\.\\d+)?\", text)\n",
    "    return numbers[-1] if numbers else None\n",
    "\n",
    "def numerically_equal(a, b, tol=1e-6):\n",
    "    try:\n",
    "        a_expr = sympify(a, evaluate=True)\n",
    "        b_expr = sympify(b, evaluate=True)\n",
    "        diff = abs(float(a_expr) - float(b_expr))\n",
    "        return diff <= tol\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "def is_correct(prediction, reference):\n",
    "    return numerically_equal(extract_last_number(prediction), extract_last_number(reference))\n",
    "\n",
    "# Generate and evaluate\n",
    "correct = 0.\n",
    "total = 0.\n",
    "\n",
    "for d in conversation_test_data:\n",
    "    prompt = d['text']\n",
    "    answer = d['answer']\n",
    "    true_answer = answer.split('####')[-1].strip()\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[-1]:], skip_special_tokens=True)\n",
    "\n",
    "    \n",
    "\n",
    "    if is_correct(response, true_answer):\n",
    "        correct += 1\n",
    "\n",
    "    total += 1\n",
    "    print ('Generated responses for:', total, 'Prediction Accuracy:', (1.*correct)/total)\n",
    "\n",
    "\n",
    "print ('Prediction Accuracy:', (1.*correct)/total)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Question:\n",
      "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
      "\n",
      "### Solution:\n",
      "Natalia sold 48/2 = <<48/2=24>>24 clips in May.\n",
      "Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\n",
      "#### 72\n"
     ]
    }
   ],
   "source": [
    "print (conversation_train_data['text'][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
