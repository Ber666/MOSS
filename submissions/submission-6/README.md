# Transformers Pretrained on Procedural Data Contain Modular Structures for Algorithmic Reasoning

## Overview

This notebook demonstrates key results from our paper on procedural pretraining. We test whether transformers pretrained on synthetic tasks contain modular inductive biases that are useful for downstream tasks.

## Contents

- `transformers_pretrained_procedural.ipynb`: Code for loading models, running the Needle-in-a-Haystack evaluation, and ablation studies.
- Includes pretrained models for **Set** and **ECA** (identical to those used in the paper).

Notebook takes ~1 hour to run this code on collab free-tier gpu.